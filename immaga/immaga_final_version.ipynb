{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d072fb6",
   "metadata": {
    "_cell_guid": "91539b2f-63da-481b-a4db-8ba08b08c4a8",
    "_uuid": "69e18ed0-5719-48ae-9d7c-e858a983fcff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:15.062521Z",
     "iopub.status.busy": "2025-04-30T15:49:15.062257Z",
     "iopub.status.idle": "2025-04-30T15:49:33.170100Z",
     "shell.execute_reply": "2025-04-30T15:49:33.169127Z"
    },
    "id": "6L6ZF0W51bNE",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "370664cd-13a8-411b-9ea7-6a09275b93dc",
    "papermill": {
     "duration": 18.113826,
     "end_time": "2025-04-30T15:49:33.171355",
     "exception": false,
     "start_time": "2025-04-30T15:49:15.057529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 0 # DEPENDENCIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, Subset, WeightedRandomSampler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673b1f2c",
   "metadata": {
    "_cell_guid": "40e931d5-91a0-429e-bdbe-41fc8306d7ca",
    "_uuid": "010b27bd-d591-4782-ad72-6aaaf63cda6d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:33.178773Z",
     "iopub.status.busy": "2025-04-30T15:49:33.178390Z",
     "iopub.status.idle": "2025-04-30T15:49:33.184424Z",
     "shell.execute_reply": "2025-04-30T15:49:33.183734Z"
    },
    "id": "TYOrJ2r11bNI",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "075d5708-412a-4d81-a093-f35ca6c900b1",
    "papermill": {
     "duration": 0.010973,
     "end_time": "2025-04-30T15:49:33.185653",
     "exception": false,
     "start_time": "2025-04-30T15:49:33.174680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "########################################## GLOBAL VARIABLES ##########################################\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 250\n",
    "PATIENCE = 150\n",
    "\n",
    "\n",
    "# 1 DATA PREPROCESSING\n",
    "MEAN = [0.5132, 0.4647, 0.4044]\n",
    "STD  = [0.2078, 0.2084, 0.2125]\n",
    "HEAVY_AUGMENT = False\n",
    "VALIDATION_PERCENTAGE = 0.1\n",
    "RANDOM_STATE = 77\n",
    "\n",
    "# 2 MODEL\n",
    "DROPOUT = 0\n",
    "\n",
    "# 4 LOSS, OPTIMIZER\n",
    "CLASS_BALANCED_WEIGHTS = True\n",
    "OLD_BALANCED_WEIGTHS = False\n",
    "GAMMA_FOCAL = 2  ##### from 1\n",
    "LR = 1e-3\n",
    "MAX_LR = 3e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# 5\n",
    "\n",
    "ALPHA_MIXUP = 0.2\n",
    "ALPHA_CUTMIX = 1.0\n",
    "PROB_MIXUP = 0.5\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce2ed2a",
   "metadata": {
    "_cell_guid": "4387b569-d82a-4e3d-ae51-8a757d226ee0",
    "_uuid": "affaa3e8-5399-41f2-a14b-dcaa885abbbb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:33.192709Z",
     "iopub.status.busy": "2025-04-30T15:49:33.192249Z",
     "iopub.status.idle": "2025-04-30T15:49:33.200689Z",
     "shell.execute_reply": "2025-04-30T15:49:33.199887Z"
    },
    "id": "GDMh45Zv1bNI",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fe5525df-4f24-4c9d-dd1a-a18b6be89390",
    "papermill": {
     "duration": 0.013179,
     "end_time": "2025-04-30T15:49:33.201820",
     "exception": false,
     "start_time": "2025-04-30T15:49:33.188641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 0 #  RETRIEVING THE IMAGES FROM KAGGLE\n",
    "\n",
    "datasets_dir = '/kaggle/input/unipd-deep-learning-2025-challenge-1'\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root: str, test: bool = False, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform or transforms.Compose([transforms.ToTensor(),])\n",
    "        self.test = test\n",
    "\n",
    "        self.img_path = osp.join(root, 'images')\n",
    "        self.targets = []\n",
    "        self.ids = []\n",
    "\n",
    "        if not test:\n",
    "            # Load images and labels\n",
    "            labels_path = osp.join(root, 'labels.csv')\n",
    "            with open(labels_path, 'r') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    image_id = row['id'].zfill(5)\n",
    "                    label = int(row['label'])\n",
    "                    self.targets.append(label)\n",
    "                    self.ids.append(image_id)\n",
    "        else:\n",
    "            # Test mode: no labels.csv\n",
    "            for fname in sorted(os.listdir(self.img_path)):\n",
    "                if fname.endswith('.jpeg'):\n",
    "                    image_id = fname[:-5].zfill(5)\n",
    "                    self.ids.append(image_id)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_id = self.ids[index]\n",
    "        img_file = osp.join(self.img_path, f'{img_id}.jpeg')\n",
    "        img = Image.open(img_file).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.test:\n",
    "            return img, img_id\n",
    "        else:\n",
    "            target = self.targets[index]\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a269eacb",
   "metadata": {
    "_cell_guid": "8080d9cf-72d7-4967-8436-ceffef559216",
    "_uuid": "48e6617f-8c9e-43b7-98c1-49cd284d99ae",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:33.209375Z",
     "iopub.status.busy": "2025-04-30T15:49:33.208683Z",
     "iopub.status.idle": "2025-04-30T15:49:33.358024Z",
     "shell.execute_reply": "2025-04-30T15:49:33.357058Z"
    },
    "id": "BxY_DEuU1bNJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f2aa576b-4565-4f21-e026-f7c0d2d77cdc",
    "papermill": {
     "duration": 0.154372,
     "end_time": "2025-04-30T15:49:33.359407",
     "exception": false,
     "start_time": "2025-04-30T15:49:33.205035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22430\n",
      "20187\n",
      "2243\n",
      "0.9\n",
      "0.8867924528301887\n",
      "141\n",
      "18\n",
      "Counter({11: 1300, 10: 1300, 9: 1300, 3: 1300, 8: 1300, 1: 1300, 0: 1300, 4: 1300, 17: 1300, 14: 1300, 12: 1300, 6: 1300, 2: 1300, 18: 1300, 19: 760, 13: 756, 5: 755, 16: 751, 7: 658, 15: 550})\n",
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1 # PREPROCESSING THE DATA\n",
    "\n",
    "########################################## DATA NORMALIZATION ##########################################\n",
    "\n",
    "normalize = transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n",
    "train_dataset_normalized = ImageDataset(osp.join(datasets_dir, 'train_dataset'), test=False, transform=normalize)\n",
    "test_dataset_normalized = ImageDataset(osp.join(datasets_dir, 'test_dataset'), test=True, transform=normalize)\n",
    "\n",
    "########################################## DATA AUGMENTATION ##########################################\n",
    "# DEFINE AUG AND STAND\n",
    "\n",
    "if HEAVY_AUGMENT:\n",
    "    augment = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(40, scale=(0.9,1.0), ratio=(1.0,1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))], p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x * (torch.rand(x.size(0), 1, 1) > 0.1).float()),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.01,0.05), ratio=(0.3, 3.3), value='random'),\n",
    "    transforms.Normalize(MEAN, STD)\n",
    "    ])\n",
    "if not HEAVY_AUGMENT:\n",
    "    augment = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.ToTensor(), transforms.Normalize(MEAN, STD)\n",
    "    ])\n",
    "\n",
    "train_dataset_augmented = ImageDataset(osp.join(datasets_dir, 'train_dataset'),\n",
    "    test=False,\n",
    "    transform=augment\n",
    ")\n",
    "\n",
    "\n",
    "########################################## SPLITTING ##########################################\n",
    "num_tot_images = len(train_dataset_augmented)\n",
    "num_val_images = int(VALIDATION_PERCENTAGE * num_tot_images)\n",
    "num_train_images = num_tot_images - num_val_images\n",
    "\n",
    "train_idxs, val_idxs = train_test_split(\n",
    "    list(range(num_train_images)),\n",
    "    test_size=num_val_images,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "train_subset = Subset(train_dataset_augmented, train_idxs)\n",
    "val_subset   = Subset(train_dataset_normalized, val_idxs)\n",
    "\n",
    "train_loader = DataLoader(train_subset, BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader   = DataLoader(val_subset,   BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset_normalized, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "########################################## CHECKING #########################################\n",
    "\n",
    "print(num_tot_images)\n",
    "print(num_train_images)\n",
    "print(num_val_images)\n",
    "print(num_train_images/(num_train_images+num_val_images))\n",
    "print(len(train_loader)/(len(train_loader)+len(val_loader)))\n",
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "dict_classes_num_images = Counter(train_dataset_normalized.targets)\n",
    "print(dict_classes_num_images)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d011b621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:33.366558Z",
     "iopub.status.busy": "2025-04-30T15:49:33.366324Z",
     "iopub.status.idle": "2025-04-30T15:49:33.378460Z",
     "shell.execute_reply": "2025-04-30T15:49:33.377764Z"
    },
    "id": "_N5x-iRW1bNJ",
    "outputId": "62db3add-08f4-4b56-a93e-6258bd1feac5",
    "papermill": {
     "duration": 0.017135,
     "end_time": "2025-04-30T15:49:33.379660",
     "exception": false,
     "start_time": "2025-04-30T15:49:33.362525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 2 # MODEL\n",
    "\n",
    "\n",
    "class ResidualLearning(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.layer1 = self._make_layer(64,  3, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        self.global_avg  = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers = [ResidualLearning(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualLearning(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.global_avg(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f71588",
   "metadata": {
    "_cell_guid": "e45a6e24-6620-4b88-a92c-954448b0467a",
    "_uuid": "55fee685-9106-4318-af8a-6cad19b464e1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:33.387375Z",
     "iopub.status.busy": "2025-04-30T15:49:33.387137Z",
     "iopub.status.idle": "2025-04-30T15:49:34.037300Z",
     "shell.execute_reply": "2025-04-30T15:49:34.036423Z"
    },
    "id": "8yCwsLuW1bNK",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "852592c0-56d0-41e0-ec21-ed953d1f496b",
    "papermill": {
     "duration": 0.656158,
     "end_time": "2025-04-30T15:49:34.038715",
     "exception": false,
     "start_time": "2025-04-30T15:49:33.382557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 3 # ASSIGNING A PHISICAL DEVICE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ImageClassifier().to(device)\n",
    "\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a0ae86",
   "metadata": {
    "_cell_guid": "a0c0f128-a454-4431-9065-c1679a91b463",
    "_uuid": "fd67b68f-dc09-4956-8947-d27092079d02",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:34.045877Z",
     "iopub.status.busy": "2025-04-30T15:49:34.045661Z",
     "iopub.status.idle": "2025-04-30T15:49:34.067757Z",
     "shell.execute_reply": "2025-04-30T15:49:34.067114Z"
    },
    "id": "AWRbL5Hm1bNL",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "eb0db97b-fb58-40ef-f752-9a0b239eaa5a",
    "papermill": {
     "duration": 0.026913,
     "end_time": "2025-04-30T15:49:34.068901",
     "exception": false,
     "start_time": "2025-04-30T15:49:34.041988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 4 # DEFINING THE LOSS\n",
    "\n",
    "########################################## COUNTING CLASSES #########################################\n",
    "dict_classes_num_images = Counter(train_dataset_augmented.targets)\n",
    "num_train_images  = sum(dict_classes_num_images.values())\n",
    "num_classes = len(dict_classes_num_images)\n",
    "\n",
    "########################################## CLASS BALANCED WEIGHT LR #########################################\n",
    "if CLASS_BALANCED_WEIGHTS:\n",
    "    beta = 0.5\n",
    "    samples_per_class = np.array([dict_classes_num_images[i] for i in range(num_classes)])\n",
    "    effective_num = 1.0 - np.power(beta, samples_per_class)\n",
    "    cb_weights    = (1.0 - beta) / effective_num\n",
    "\n",
    "    cb_weights = cb_weights / cb_weights.mean()\n",
    "    weights = torch.tensor(cb_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "########################################## OLD WEIGHT LR #########################################\n",
    "if OLD_BALANCED_WEIGTHS:\n",
    "    class_weights = [num_train_images/(num_classes * dict_classes_num_images[i]) for i in range(num_classes)]\n",
    "    weights = torch.tensor(class_weights, device=device)\n",
    "\n",
    "########################################## FOCAL LOSS #########################################\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(weight=self.alpha, reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "########################################## LOSS #########################################\n",
    "loss_function = FocalLoss(alpha=weights, gamma=GAMMA_FOCAL).to(device)\n",
    "\n",
    "########################################## OPTIMIZER #########################################\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "\n",
    "########################################## SCHEDULER #########################################\n",
    "\n",
    "steps = len(train_loader) * (NUM_EPOCHS)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-3,\n",
    "    total_steps=steps,\n",
    "    pct_start=0.2,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd3ab44",
   "metadata": {
    "_cell_guid": "08c9ce93-0669-4f07-bf29-81a10eb3646c",
    "_uuid": "6c9ea4f1-3b04-440f-8b32-ef0b65b7f335",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:34.076604Z",
     "iopub.status.busy": "2025-04-30T15:49:34.076361Z",
     "iopub.status.idle": "2025-04-30T15:49:34.098386Z",
     "shell.execute_reply": "2025-04-30T15:49:34.097693Z"
    },
    "id": "IoYOnvGw1bNN",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6b41f01c-069a-4b64-af88-df10ff8b04b8",
    "papermill": {
     "duration": 0.027448,
     "end_time": "2025-04-30T15:49:34.099643",
     "exception": false,
     "start_time": "2025-04-30T15:49:34.072195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 5 # DEF TRAINING STOP ACC\n",
    "\n",
    "########################################## MIX UP #########################################\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda.\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
    "    batch_size = x.size(0)\n",
    "    idx = torch.randperm(batch_size, device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "########################################## CUT MIX #########################################\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform center position\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(x.size(0)).to(x.device)\n",
    "    y_a, y_b = y, y[rand_index]\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "########################################## ALTERNATING THEM #########################################\n",
    "\n",
    "def mixup_or_cutmix(images, labels, alpha_mixup, alpha_cutmix, prob_mixup=0.5):\n",
    "    if random.random() < prob_mixup:\n",
    "        return mixup_data(images, labels, alpha=alpha_mixup)\n",
    "    else:\n",
    "        return cutmix_data(images, labels, alpha=alpha_cutmix)\n",
    "\n",
    "########################################## DEF TRAINING #########################################\n",
    "def train(model,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          optimizer,\n",
    "          scheduler,\n",
    "          loss_function,\n",
    "          device,\n",
    "          num_epochs,\n",
    "          patience):\n",
    "\n",
    "    best_acc = 0.0\n",
    "    not_improved_epochs_count = 0\n",
    "\n",
    "    for current_epoch in range(1, num_epochs+1):\n",
    "\n",
    "        if current_epoch == 31:\n",
    "            print(f\"⚡ Epoch 31 reached — enabling HEAVY_AUGMENT now and rebuilding train_loader…\")\n",
    "            HEAVY_AUGMENT = True\n",
    "            if HEAVY_AUGMENT:\n",
    "                augment = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(40, scale=(0.9,1.0), ratio=(1.0,1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))], p=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x * (torch.rand(x.size(0), 1, 1) > 0.1).float()),\n",
    "                transforms.RandomErasing(p=0.3, scale=(0.01,0.05), ratio=(0.3, 3.3), value='random'),\n",
    "                transforms.Normalize(MEAN, STD)\n",
    "                ])\n",
    "            if not HEAVY_AUGMENT:\n",
    "              augment = transforms.Compose([\n",
    "                  transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(15),\n",
    "                  transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "                  transforms.ToTensor(), transforms.Normalize(MEAN, STD)\n",
    "              ])\n",
    "\n",
    "            train_dataset_augmented = ImageDataset(osp.join(datasets_dir, 'train_dataset'), test=False, transform=augment)\n",
    "            train_subset = Subset(train_dataset_augmented, train_idxs)\n",
    "            train_loader = DataLoader(train_subset, BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        nailed_train = 0\n",
    "        seen_train = 0\n",
    "        losses_train = []\n",
    "\n",
    "\n",
    "        for images, true_labels in train_loader:\n",
    "            images, true_labels = images.to(device), true_labels.to(device)\n",
    "\n",
    "            mixed_x, y_a, y_b, lam = mixup_or_cutmix(images, true_labels, alpha_mixup=ALPHA_MIXUP, alpha_cutmix=ALPHA_CUTMIX, prob_mixup=PROB_MIXUP)\n",
    "\n",
    "            predicted_logits = model(mixed_x)\n",
    "\n",
    "            loss = ( lam * loss_function(predicted_logits, y_a) + (1-lam) * loss_function(predicted_logits, y_b) )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            losses_train.append(loss.item())\n",
    "\n",
    "            predicted_labels = predicted_logits.argmax(dim=1)\n",
    "            nailed_train += (predicted_labels == true_labels).sum().item()\n",
    "            seen_train += true_labels.size(0)\n",
    "\n",
    "        accuracy_train = nailed_train / seen_train\n",
    "        mean_loss_train = np.mean(losses_train)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        nailed_val = 0\n",
    "        seen_val = 0\n",
    "        losses_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for images, true_labels in val_loader:\n",
    "                images, true_labels = images.to(device), true_labels.to(device)\n",
    "                predicted_logits = model(images)\n",
    "                loss = loss_function(predicted_logits, true_labels)\n",
    "                losses_val.append(loss.item())\n",
    "                predicted_labels = predicted_logits.argmax(dim=1)\n",
    "                nailed_val += (predicted_labels == true_labels).sum().item()\n",
    "                seen_val += true_labels.size(0)\n",
    "\n",
    "        accuracy_val = nailed_val / seen_val\n",
    "        mean_loss_val = np.mean(losses_val)\n",
    "\n",
    "        print(f\"Epoch {current_epoch:03d} | \"\n",
    "              f\"Train Acc: {accuracy_train:.4f} | \"\n",
    "              f\"Val   Acc: {accuracy_val:.4f} |\"\n",
    "              f\"Loss: {mean_loss_val:.4f} |\")\n",
    "\n",
    "        if accuracy_val > best_acc:\n",
    "            best_acc = accuracy_val\n",
    "            not_improved_epochs_count = 0\n",
    "            torch.save({'state_dict': model.state_dict(),'val_acc':    accuracy_val}, 'final_model.pth')\n",
    "        else:\n",
    "            not_improved_epochs_count += 1\n",
    "            if not_improved_epochs_count >= patience:\n",
    "                print(f\"Stopping early at epoch {current_epoch}\")\n",
    "                model.load_state_dict(torch.load('final_model.pth')['state_dict'])\n",
    "                break\n",
    "    print(f'best acc: {best_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ed4f56",
   "metadata": {
    "_cell_guid": "9df1cc9b-4abe-40be-af51-f4febce5806c",
    "_uuid": "dc476c4c-2310-47b0-96de-aec32fda8957",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T15:49:34.107166Z",
     "iopub.status.busy": "2025-04-30T15:49:34.106920Z",
     "iopub.status.idle": "2025-04-30T19:06:21.306907Z",
     "shell.execute_reply": "2025-04-30T19:06:21.305774Z"
    },
    "id": "mZZM25Y21bNO",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "50382efe-1fc0-4ea6-9073-a80bd212c0bb",
    "papermill": {
     "duration": 11807.22125,
     "end_time": "2025-04-30T19:06:21.324332",
     "exception": false,
     "start_time": "2025-04-30T15:49:34.103082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Acc: 0.1600 | Val   Acc: 0.2974 |Loss: 1.7130 |\n",
      "Epoch 002 | Train Acc: 0.2105 | Val   Acc: 0.3388 |Loss: 1.5462 |\n",
      "Epoch 003 | Train Acc: 0.2454 | Val   Acc: 0.3919 |Loss: 1.4557 |\n",
      "Epoch 004 | Train Acc: 0.2684 | Val   Acc: 0.4646 |Loss: 1.1473 |\n",
      "Epoch 005 | Train Acc: 0.2742 | Val   Acc: 0.4539 |Loss: 1.1914 |\n",
      "Epoch 006 | Train Acc: 0.3231 | Val   Acc: 0.4128 |Loss: 1.4609 |\n",
      "Epoch 007 | Train Acc: 0.2972 | Val   Acc: 0.4360 |Loss: 1.4920 |\n",
      "Epoch 008 | Train Acc: 0.3292 | Val   Acc: 0.5207 |Loss: 1.0959 |\n",
      "Epoch 009 | Train Acc: 0.3279 | Val   Acc: 0.5247 |Loss: 0.9826 |\n",
      "Epoch 010 | Train Acc: 0.3423 | Val   Acc: 0.4895 |Loss: 1.1570 |\n",
      "Epoch 011 | Train Acc: 0.3349 | Val   Acc: 0.5230 |Loss: 0.9804 |\n",
      "Epoch 012 | Train Acc: 0.3543 | Val   Acc: 0.6005 |Loss: 0.7929 |\n",
      "Epoch 013 | Train Acc: 0.3961 | Val   Acc: 0.6028 |Loss: 0.8162 |\n",
      "Epoch 014 | Train Acc: 0.3902 | Val   Acc: 0.5921 |Loss: 0.8309 |\n",
      "Epoch 015 | Train Acc: 0.3565 | Val   Acc: 0.5640 |Loss: 0.9434 |\n",
      "Epoch 016 | Train Acc: 0.3991 | Val   Acc: 0.5653 |Loss: 0.9434 |\n",
      "Epoch 017 | Train Acc: 0.3965 | Val   Acc: 0.6202 |Loss: 0.7766 |\n",
      "Epoch 018 | Train Acc: 0.3911 | Val   Acc: 0.5564 |Loss: 0.9153 |\n",
      "Epoch 019 | Train Acc: 0.4169 | Val   Acc: 0.5582 |Loss: 0.8711 |\n",
      "Epoch 020 | Train Acc: 0.3799 | Val   Acc: 0.6509 |Loss: 0.6730 |\n",
      "Epoch 021 | Train Acc: 0.4010 | Val   Acc: 0.6068 |Loss: 0.8246 |\n",
      "Epoch 022 | Train Acc: 0.3877 | Val   Acc: 0.6353 |Loss: 0.8129 |\n",
      "Epoch 023 | Train Acc: 0.3912 | Val   Acc: 0.6081 |Loss: 0.8130 |\n",
      "Epoch 024 | Train Acc: 0.4119 | Val   Acc: 0.6402 |Loss: 0.7174 |\n",
      "Epoch 025 | Train Acc: 0.4234 | Val   Acc: 0.5858 |Loss: 0.8479 |\n",
      "Epoch 026 | Train Acc: 0.4292 | Val   Acc: 0.6393 |Loss: 0.7274 |\n",
      "Epoch 027 | Train Acc: 0.4457 | Val   Acc: 0.6580 |Loss: 0.7168 |\n",
      "Epoch 028 | Train Acc: 0.4012 | Val   Acc: 0.6353 |Loss: 0.7303 |\n",
      "Epoch 029 | Train Acc: 0.4271 | Val   Acc: 0.5698 |Loss: 0.9160 |\n",
      "Epoch 030 | Train Acc: 0.4260 | Val   Acc: 0.6634 |Loss: 0.6831 |\n",
      "⚡ Epoch 31 reached — enabling HEAVY_AUGMENT now and rebuilding train_loader…\n",
      "Epoch 031 | Train Acc: 0.3856 | Val   Acc: 0.5823 |Loss: 0.9468 |\n",
      "Epoch 032 | Train Acc: 0.3869 | Val   Acc: 0.6148 |Loss: 0.7812 |\n",
      "Epoch 033 | Train Acc: 0.4006 | Val   Acc: 0.5938 |Loss: 0.8433 |\n",
      "Epoch 034 | Train Acc: 0.4185 | Val   Acc: 0.6286 |Loss: 0.7613 |\n",
      "Epoch 035 | Train Acc: 0.4172 | Val   Acc: 0.6193 |Loss: 0.8365 |\n",
      "Epoch 036 | Train Acc: 0.4466 | Val   Acc: 0.6888 |Loss: 0.5807 |\n",
      "Epoch 037 | Train Acc: 0.4383 | Val   Acc: 0.6090 |Loss: 0.8805 |\n",
      "Epoch 038 | Train Acc: 0.4261 | Val   Acc: 0.6777 |Loss: 0.6541 |\n",
      "Epoch 039 | Train Acc: 0.4691 | Val   Acc: 0.6433 |Loss: 0.6925 |\n",
      "Epoch 040 | Train Acc: 0.4464 | Val   Acc: 0.6692 |Loss: 0.6626 |\n",
      "Epoch 041 | Train Acc: 0.4872 | Val   Acc: 0.6665 |Loss: 0.6832 |\n",
      "Epoch 042 | Train Acc: 0.4474 | Val   Acc: 0.6973 |Loss: 0.5960 |\n",
      "Epoch 043 | Train Acc: 0.4651 | Val   Acc: 0.6844 |Loss: 0.5916 |\n",
      "Epoch 044 | Train Acc: 0.4281 | Val   Acc: 0.6687 |Loss: 0.6874 |\n",
      "Epoch 045 | Train Acc: 0.4516 | Val   Acc: 0.6852 |Loss: 0.5801 |\n",
      "Epoch 046 | Train Acc: 0.4126 | Val   Acc: 0.7093 |Loss: 0.5699 |\n",
      "Epoch 047 | Train Acc: 0.4303 | Val   Acc: 0.7058 |Loss: 0.5823 |\n",
      "Epoch 048 | Train Acc: 0.4469 | Val   Acc: 0.6447 |Loss: 0.6990 |\n",
      "Epoch 049 | Train Acc: 0.4472 | Val   Acc: 0.7370 |Loss: 0.5207 |\n",
      "Epoch 050 | Train Acc: 0.4281 | Val   Acc: 0.6875 |Loss: 0.5801 |\n",
      "Epoch 051 | Train Acc: 0.4811 | Val   Acc: 0.7133 |Loss: 0.5231 |\n",
      "Epoch 052 | Train Acc: 0.4676 | Val   Acc: 0.6986 |Loss: 0.5856 |\n",
      "Epoch 053 | Train Acc: 0.4723 | Val   Acc: 0.7049 |Loss: 0.5557 |\n",
      "Epoch 054 | Train Acc: 0.4656 | Val   Acc: 0.7173 |Loss: 0.5247 |\n",
      "Epoch 055 | Train Acc: 0.4626 | Val   Acc: 0.7303 |Loss: 0.4984 |\n",
      "Epoch 056 | Train Acc: 0.5075 | Val   Acc: 0.7165 |Loss: 0.5499 |\n",
      "Epoch 057 | Train Acc: 0.4239 | Val   Acc: 0.7356 |Loss: 0.5027 |\n",
      "Epoch 058 | Train Acc: 0.4981 | Val   Acc: 0.7049 |Loss: 0.5339 |\n",
      "Epoch 059 | Train Acc: 0.5113 | Val   Acc: 0.7236 |Loss: 0.5473 |\n",
      "Epoch 060 | Train Acc: 0.4663 | Val   Acc: 0.7459 |Loss: 0.4876 |\n",
      "Epoch 061 | Train Acc: 0.4722 | Val   Acc: 0.7352 |Loss: 0.5127 |\n",
      "Epoch 062 | Train Acc: 0.4711 | Val   Acc: 0.7343 |Loss: 0.5357 |\n",
      "Epoch 063 | Train Acc: 0.5123 | Val   Acc: 0.7013 |Loss: 0.6121 |\n",
      "Epoch 064 | Train Acc: 0.4955 | Val   Acc: 0.7334 |Loss: 0.5058 |\n",
      "Epoch 065 | Train Acc: 0.5115 | Val   Acc: 0.7436 |Loss: 0.5159 |\n",
      "Epoch 066 | Train Acc: 0.5052 | Val   Acc: 0.7334 |Loss: 0.5190 |\n",
      "Epoch 067 | Train Acc: 0.5173 | Val   Acc: 0.7338 |Loss: 0.5113 |\n",
      "Epoch 068 | Train Acc: 0.4629 | Val   Acc: 0.7486 |Loss: 0.4910 |\n",
      "Epoch 069 | Train Acc: 0.5633 | Val   Acc: 0.7548 |Loss: 0.4853 |\n",
      "Epoch 070 | Train Acc: 0.4680 | Val   Acc: 0.7329 |Loss: 0.5306 |\n",
      "Epoch 071 | Train Acc: 0.5087 | Val   Acc: 0.7276 |Loss: 0.5259 |\n",
      "Epoch 072 | Train Acc: 0.5184 | Val   Acc: 0.7370 |Loss: 0.5351 |\n",
      "Epoch 073 | Train Acc: 0.5266 | Val   Acc: 0.7414 |Loss: 0.5028 |\n",
      "Epoch 074 | Train Acc: 0.5150 | Val   Acc: 0.7472 |Loss: 0.5043 |\n",
      "Epoch 075 | Train Acc: 0.4983 | Val   Acc: 0.7218 |Loss: 0.5789 |\n",
      "Epoch 076 | Train Acc: 0.5199 | Val   Acc: 0.7481 |Loss: 0.5018 |\n",
      "Epoch 077 | Train Acc: 0.5445 | Val   Acc: 0.7200 |Loss: 0.5473 |\n",
      "Epoch 078 | Train Acc: 0.4652 | Val   Acc: 0.6580 |Loss: 0.7500 |\n",
      "Epoch 079 | Train Acc: 0.5521 | Val   Acc: 0.7307 |Loss: 0.5583 |\n",
      "Epoch 080 | Train Acc: 0.5730 | Val   Acc: 0.7312 |Loss: 0.5578 |\n",
      "Epoch 081 | Train Acc: 0.5727 | Val   Acc: 0.7642 |Loss: 0.4849 |\n",
      "Epoch 082 | Train Acc: 0.5418 | Val   Acc: 0.7535 |Loss: 0.4953 |\n",
      "Epoch 083 | Train Acc: 0.5022 | Val   Acc: 0.7298 |Loss: 0.5556 |\n",
      "Epoch 084 | Train Acc: 0.5154 | Val   Acc: 0.7588 |Loss: 0.5087 |\n",
      "Epoch 085 | Train Acc: 0.5546 | Val   Acc: 0.7561 |Loss: 0.5082 |\n",
      "Epoch 086 | Train Acc: 0.5483 | Val   Acc: 0.7441 |Loss: 0.5171 |\n",
      "Epoch 087 | Train Acc: 0.5348 | Val   Acc: 0.7379 |Loss: 0.5135 |\n",
      "Epoch 088 | Train Acc: 0.5621 | Val   Acc: 0.7289 |Loss: 0.5847 |\n",
      "Epoch 089 | Train Acc: 0.5789 | Val   Acc: 0.7432 |Loss: 0.5258 |\n",
      "Epoch 090 | Train Acc: 0.5816 | Val   Acc: 0.7432 |Loss: 0.5297 |\n",
      "Epoch 091 | Train Acc: 0.5426 | Val   Acc: 0.7222 |Loss: 0.5627 |\n",
      "Epoch 092 | Train Acc: 0.5822 | Val   Acc: 0.7526 |Loss: 0.4968 |\n",
      "Epoch 093 | Train Acc: 0.6079 | Val   Acc: 0.7494 |Loss: 0.5247 |\n",
      "Epoch 094 | Train Acc: 0.6023 | Val   Acc: 0.7387 |Loss: 0.5450 |\n",
      "Epoch 095 | Train Acc: 0.5787 | Val   Acc: 0.7486 |Loss: 0.5126 |\n",
      "Epoch 096 | Train Acc: 0.5656 | Val   Acc: 0.7530 |Loss: 0.5336 |\n",
      "Epoch 097 | Train Acc: 0.5558 | Val   Acc: 0.7704 |Loss: 0.4790 |\n",
      "Epoch 098 | Train Acc: 0.5415 | Val   Acc: 0.7535 |Loss: 0.5146 |\n",
      "Epoch 099 | Train Acc: 0.5348 | Val   Acc: 0.7441 |Loss: 0.5186 |\n",
      "Epoch 100 | Train Acc: 0.5832 | Val   Acc: 0.7597 |Loss: 0.4872 |\n",
      "Epoch 101 | Train Acc: 0.5468 | Val   Acc: 0.7530 |Loss: 0.4963 |\n",
      "Epoch 102 | Train Acc: 0.4968 | Val   Acc: 0.7503 |Loss: 0.4878 |\n",
      "Epoch 103 | Train Acc: 0.5479 | Val   Acc: 0.7521 |Loss: 0.4890 |\n",
      "Epoch 104 | Train Acc: 0.5839 | Val   Acc: 0.7468 |Loss: 0.5104 |\n",
      "Epoch 105 | Train Acc: 0.5938 | Val   Acc: 0.7584 |Loss: 0.5016 |\n",
      "Epoch 106 | Train Acc: 0.5644 | Val   Acc: 0.7459 |Loss: 0.5470 |\n",
      "Epoch 107 | Train Acc: 0.5565 | Val   Acc: 0.7463 |Loss: 0.5104 |\n",
      "Epoch 108 | Train Acc: 0.5196 | Val   Acc: 0.7597 |Loss: 0.5095 |\n",
      "Epoch 109 | Train Acc: 0.5524 | Val   Acc: 0.7517 |Loss: 0.5082 |\n",
      "Epoch 110 | Train Acc: 0.5533 | Val   Acc: 0.7503 |Loss: 0.4877 |\n",
      "Epoch 111 | Train Acc: 0.5762 | Val   Acc: 0.7633 |Loss: 0.4865 |\n",
      "Epoch 112 | Train Acc: 0.6276 | Val   Acc: 0.7601 |Loss: 0.5318 |\n",
      "Epoch 113 | Train Acc: 0.6178 | Val   Acc: 0.7601 |Loss: 0.4892 |\n",
      "Epoch 114 | Train Acc: 0.6150 | Val   Acc: 0.7561 |Loss: 0.4945 |\n",
      "Epoch 115 | Train Acc: 0.6492 | Val   Acc: 0.7494 |Loss: 0.5173 |\n",
      "Epoch 116 | Train Acc: 0.5667 | Val   Acc: 0.7468 |Loss: 0.5404 |\n",
      "Epoch 117 | Train Acc: 0.5353 | Val   Acc: 0.7503 |Loss: 0.5092 |\n",
      "Epoch 118 | Train Acc: 0.5915 | Val   Acc: 0.7610 |Loss: 0.5070 |\n",
      "Epoch 119 | Train Acc: 0.5893 | Val   Acc: 0.7650 |Loss: 0.4842 |\n",
      "Epoch 120 | Train Acc: 0.5688 | Val   Acc: 0.7526 |Loss: 0.4959 |\n",
      "Epoch 121 | Train Acc: 0.5284 | Val   Acc: 0.7530 |Loss: 0.4992 |\n",
      "Epoch 122 | Train Acc: 0.5743 | Val   Acc: 0.7762 |Loss: 0.4646 |\n",
      "Epoch 123 | Train Acc: 0.5789 | Val   Acc: 0.7588 |Loss: 0.4949 |\n",
      "Epoch 124 | Train Acc: 0.5689 | Val   Acc: 0.7606 |Loss: 0.4996 |\n",
      "Epoch 125 | Train Acc: 0.5536 | Val   Acc: 0.7552 |Loss: 0.5223 |\n",
      "Epoch 126 | Train Acc: 0.5925 | Val   Acc: 0.7575 |Loss: 0.4977 |\n",
      "Epoch 127 | Train Acc: 0.5640 | Val   Acc: 0.7615 |Loss: 0.4795 |\n",
      "Epoch 128 | Train Acc: 0.5970 | Val   Acc: 0.7673 |Loss: 0.5115 |\n",
      "Epoch 129 | Train Acc: 0.5491 | Val   Acc: 0.7757 |Loss: 0.4784 |\n",
      "Epoch 130 | Train Acc: 0.5821 | Val   Acc: 0.7601 |Loss: 0.5052 |\n",
      "Epoch 131 | Train Acc: 0.5812 | Val   Acc: 0.7552 |Loss: 0.4821 |\n",
      "Epoch 132 | Train Acc: 0.6459 | Val   Acc: 0.7757 |Loss: 0.4794 |\n",
      "Epoch 133 | Train Acc: 0.5767 | Val   Acc: 0.7691 |Loss: 0.5040 |\n",
      "Epoch 134 | Train Acc: 0.6179 | Val   Acc: 0.7512 |Loss: 0.4911 |\n",
      "Epoch 135 | Train Acc: 0.6543 | Val   Acc: 0.7749 |Loss: 0.4870 |\n",
      "Epoch 136 | Train Acc: 0.6080 | Val   Acc: 0.7762 |Loss: 0.4863 |\n",
      "Epoch 137 | Train Acc: 0.5882 | Val   Acc: 0.7762 |Loss: 0.4629 |\n",
      "Epoch 138 | Train Acc: 0.6088 | Val   Acc: 0.7646 |Loss: 0.5079 |\n",
      "Epoch 139 | Train Acc: 0.5938 | Val   Acc: 0.7789 |Loss: 0.4792 |\n",
      "Epoch 140 | Train Acc: 0.5744 | Val   Acc: 0.7677 |Loss: 0.4875 |\n",
      "Epoch 141 | Train Acc: 0.6335 | Val   Acc: 0.7784 |Loss: 0.4907 |\n",
      "Epoch 142 | Train Acc: 0.5993 | Val   Acc: 0.7722 |Loss: 0.5025 |\n",
      "Epoch 143 | Train Acc: 0.5897 | Val   Acc: 0.7695 |Loss: 0.4958 |\n",
      "Epoch 144 | Train Acc: 0.5762 | Val   Acc: 0.7642 |Loss: 0.4706 |\n",
      "Epoch 145 | Train Acc: 0.5382 | Val   Acc: 0.7740 |Loss: 0.4545 |\n",
      "Epoch 146 | Train Acc: 0.6358 | Val   Acc: 0.7650 |Loss: 0.4749 |\n",
      "Epoch 147 | Train Acc: 0.6160 | Val   Acc: 0.7766 |Loss: 0.4520 |\n",
      "Epoch 148 | Train Acc: 0.6625 | Val   Acc: 0.7717 |Loss: 0.4831 |\n",
      "Epoch 149 | Train Acc: 0.6413 | Val   Acc: 0.7731 |Loss: 0.4784 |\n",
      "Epoch 150 | Train Acc: 0.6126 | Val   Acc: 0.7722 |Loss: 0.4479 |\n",
      "Epoch 151 | Train Acc: 0.6327 | Val   Acc: 0.7771 |Loss: 0.4934 |\n",
      "Epoch 152 | Train Acc: 0.5438 | Val   Acc: 0.7708 |Loss: 0.4898 |\n",
      "Epoch 153 | Train Acc: 0.6410 | Val   Acc: 0.7789 |Loss: 0.4949 |\n",
      "Epoch 154 | Train Acc: 0.6718 | Val   Acc: 0.7789 |Loss: 0.4850 |\n",
      "Epoch 155 | Train Acc: 0.6127 | Val   Acc: 0.7842 |Loss: 0.4627 |\n",
      "Epoch 156 | Train Acc: 0.6322 | Val   Acc: 0.7726 |Loss: 0.4934 |\n",
      "Epoch 157 | Train Acc: 0.6320 | Val   Acc: 0.7789 |Loss: 0.4662 |\n",
      "Epoch 158 | Train Acc: 0.5999 | Val   Acc: 0.7789 |Loss: 0.4995 |\n",
      "Epoch 159 | Train Acc: 0.5932 | Val   Acc: 0.7838 |Loss: 0.4793 |\n",
      "Epoch 160 | Train Acc: 0.5730 | Val   Acc: 0.7784 |Loss: 0.4761 |\n",
      "Epoch 161 | Train Acc: 0.6340 | Val   Acc: 0.7851 |Loss: 0.4839 |\n",
      "Epoch 162 | Train Acc: 0.6351 | Val   Acc: 0.7802 |Loss: 0.4949 |\n",
      "Epoch 163 | Train Acc: 0.6698 | Val   Acc: 0.7708 |Loss: 0.4827 |\n",
      "Epoch 164 | Train Acc: 0.5907 | Val   Acc: 0.7847 |Loss: 0.4827 |\n",
      "Epoch 165 | Train Acc: 0.6020 | Val   Acc: 0.7909 |Loss: 0.4678 |\n",
      "Epoch 166 | Train Acc: 0.6263 | Val   Acc: 0.7847 |Loss: 0.4992 |\n",
      "Epoch 167 | Train Acc: 0.5496 | Val   Acc: 0.7847 |Loss: 0.4610 |\n",
      "Epoch 168 | Train Acc: 0.5978 | Val   Acc: 0.7820 |Loss: 0.4744 |\n",
      "Epoch 169 | Train Acc: 0.6247 | Val   Acc: 0.7815 |Loss: 0.4801 |\n",
      "Epoch 170 | Train Acc: 0.6373 | Val   Acc: 0.7860 |Loss: 0.4557 |\n",
      "Epoch 171 | Train Acc: 0.6199 | Val   Acc: 0.7771 |Loss: 0.4829 |\n",
      "Epoch 172 | Train Acc: 0.5882 | Val   Acc: 0.7780 |Loss: 0.4634 |\n",
      "Epoch 173 | Train Acc: 0.6290 | Val   Acc: 0.7891 |Loss: 0.4862 |\n",
      "Epoch 174 | Train Acc: 0.6434 | Val   Acc: 0.7918 |Loss: 0.4697 |\n",
      "Epoch 175 | Train Acc: 0.6521 | Val   Acc: 0.7860 |Loss: 0.4440 |\n",
      "Epoch 176 | Train Acc: 0.6119 | Val   Acc: 0.7851 |Loss: 0.4677 |\n",
      "Epoch 177 | Train Acc: 0.6225 | Val   Acc: 0.7780 |Loss: 0.4848 |\n",
      "Epoch 178 | Train Acc: 0.5735 | Val   Acc: 0.7878 |Loss: 0.4723 |\n",
      "Epoch 179 | Train Acc: 0.5673 | Val   Acc: 0.7900 |Loss: 0.4456 |\n",
      "Epoch 180 | Train Acc: 0.5898 | Val   Acc: 0.7802 |Loss: 0.4785 |\n",
      "Epoch 181 | Train Acc: 0.6643 | Val   Acc: 0.7882 |Loss: 0.4546 |\n",
      "Epoch 182 | Train Acc: 0.6127 | Val   Acc: 0.7842 |Loss: 0.4602 |\n",
      "Epoch 183 | Train Acc: 0.5736 | Val   Acc: 0.7766 |Loss: 0.4515 |\n",
      "Epoch 184 | Train Acc: 0.5870 | Val   Acc: 0.7918 |Loss: 0.4313 |\n",
      "Epoch 185 | Train Acc: 0.6098 | Val   Acc: 0.7864 |Loss: 0.4702 |\n",
      "Epoch 186 | Train Acc: 0.6549 | Val   Acc: 0.7940 |Loss: 0.4296 |\n",
      "Epoch 187 | Train Acc: 0.6723 | Val   Acc: 0.7931 |Loss: 0.4421 |\n",
      "Epoch 188 | Train Acc: 0.6144 | Val   Acc: 0.7931 |Loss: 0.4384 |\n",
      "Epoch 189 | Train Acc: 0.6822 | Val   Acc: 0.7887 |Loss: 0.4625 |\n",
      "Epoch 190 | Train Acc: 0.6393 | Val   Acc: 0.7869 |Loss: 0.4368 |\n",
      "Epoch 191 | Train Acc: 0.6100 | Val   Acc: 0.7891 |Loss: 0.4374 |\n",
      "Epoch 192 | Train Acc: 0.6180 | Val   Acc: 0.7909 |Loss: 0.4568 |\n",
      "Epoch 193 | Train Acc: 0.5763 | Val   Acc: 0.7927 |Loss: 0.4198 |\n",
      "Epoch 194 | Train Acc: 0.6409 | Val   Acc: 0.7931 |Loss: 0.4589 |\n",
      "Epoch 195 | Train Acc: 0.6150 | Val   Acc: 0.7878 |Loss: 0.4573 |\n",
      "Epoch 196 | Train Acc: 0.5463 | Val   Acc: 0.7856 |Loss: 0.4601 |\n",
      "Epoch 197 | Train Acc: 0.5826 | Val   Acc: 0.7976 |Loss: 0.4444 |\n",
      "Epoch 198 | Train Acc: 0.6261 | Val   Acc: 0.7963 |Loss: 0.4357 |\n",
      "Epoch 199 | Train Acc: 0.6110 | Val   Acc: 0.7963 |Loss: 0.4337 |\n",
      "Epoch 200 | Train Acc: 0.6149 | Val   Acc: 0.7963 |Loss: 0.4390 |\n",
      "Epoch 201 | Train Acc: 0.6461 | Val   Acc: 0.7985 |Loss: 0.4494 |\n",
      "Epoch 202 | Train Acc: 0.6583 | Val   Acc: 0.7963 |Loss: 0.4400 |\n",
      "Epoch 203 | Train Acc: 0.5753 | Val   Acc: 0.7985 |Loss: 0.4250 |\n",
      "Epoch 204 | Train Acc: 0.6546 | Val   Acc: 0.7949 |Loss: 0.4541 |\n",
      "Epoch 205 | Train Acc: 0.6344 | Val   Acc: 0.8021 |Loss: 0.4248 |\n",
      "Epoch 206 | Train Acc: 0.6173 | Val   Acc: 0.7954 |Loss: 0.4318 |\n",
      "Epoch 207 | Train Acc: 0.6674 | Val   Acc: 0.7994 |Loss: 0.4422 |\n",
      "Epoch 208 | Train Acc: 0.6205 | Val   Acc: 0.8003 |Loss: 0.4309 |\n",
      "Epoch 209 | Train Acc: 0.6852 | Val   Acc: 0.8007 |Loss: 0.4273 |\n",
      "Epoch 210 | Train Acc: 0.6496 | Val   Acc: 0.8003 |Loss: 0.4309 |\n",
      "Epoch 211 | Train Acc: 0.5882 | Val   Acc: 0.7940 |Loss: 0.4610 |\n",
      "Epoch 212 | Train Acc: 0.6664 | Val   Acc: 0.7945 |Loss: 0.4249 |\n",
      "Epoch 213 | Train Acc: 0.6392 | Val   Acc: 0.7994 |Loss: 0.4434 |\n",
      "Epoch 214 | Train Acc: 0.6180 | Val   Acc: 0.7971 |Loss: 0.4160 |\n",
      "Epoch 215 | Train Acc: 0.5981 | Val   Acc: 0.7936 |Loss: 0.4422 |\n",
      "Epoch 216 | Train Acc: 0.6134 | Val   Acc: 0.7954 |Loss: 0.4493 |\n",
      "Epoch 217 | Train Acc: 0.6597 | Val   Acc: 0.7976 |Loss: 0.4409 |\n",
      "Epoch 218 | Train Acc: 0.5610 | Val   Acc: 0.8043 |Loss: 0.4293 |\n",
      "Epoch 219 | Train Acc: 0.6525 | Val   Acc: 0.7958 |Loss: 0.4239 |\n",
      "Epoch 220 | Train Acc: 0.6461 | Val   Acc: 0.8003 |Loss: 0.4289 |\n",
      "Epoch 221 | Train Acc: 0.6555 | Val   Acc: 0.8021 |Loss: 0.4095 |\n",
      "Epoch 222 | Train Acc: 0.6552 | Val   Acc: 0.7980 |Loss: 0.4161 |\n",
      "Epoch 223 | Train Acc: 0.5943 | Val   Acc: 0.7936 |Loss: 0.4485 |\n",
      "Epoch 224 | Train Acc: 0.6383 | Val   Acc: 0.7989 |Loss: 0.4205 |\n",
      "Epoch 225 | Train Acc: 0.6544 | Val   Acc: 0.7989 |Loss: 0.4417 |\n",
      "Epoch 226 | Train Acc: 0.6077 | Val   Acc: 0.7940 |Loss: 0.4315 |\n",
      "Epoch 227 | Train Acc: 0.6114 | Val   Acc: 0.8021 |Loss: 0.4297 |\n",
      "Epoch 228 | Train Acc: 0.6349 | Val   Acc: 0.8007 |Loss: 0.4261 |\n",
      "Epoch 229 | Train Acc: 0.5824 | Val   Acc: 0.8025 |Loss: 0.4307 |\n",
      "Epoch 230 | Train Acc: 0.6083 | Val   Acc: 0.7989 |Loss: 0.4211 |\n",
      "Epoch 231 | Train Acc: 0.6390 | Val   Acc: 0.7980 |Loss: 0.4324 |\n",
      "Epoch 232 | Train Acc: 0.6885 | Val   Acc: 0.7985 |Loss: 0.4323 |\n",
      "Epoch 233 | Train Acc: 0.6582 | Val   Acc: 0.7985 |Loss: 0.4227 |\n",
      "Epoch 234 | Train Acc: 0.6382 | Val   Acc: 0.7985 |Loss: 0.4311 |\n",
      "Epoch 235 | Train Acc: 0.6117 | Val   Acc: 0.7998 |Loss: 0.4386 |\n",
      "Epoch 236 | Train Acc: 0.6348 | Val   Acc: 0.8021 |Loss: 0.4403 |\n",
      "Epoch 237 | Train Acc: 0.6951 | Val   Acc: 0.8003 |Loss: 0.4169 |\n",
      "Epoch 238 | Train Acc: 0.6216 | Val   Acc: 0.8007 |Loss: 0.4033 |\n",
      "Epoch 239 | Train Acc: 0.6314 | Val   Acc: 0.8021 |Loss: 0.4300 |\n",
      "Epoch 240 | Train Acc: 0.6264 | Val   Acc: 0.8016 |Loss: 0.4252 |\n",
      "Epoch 241 | Train Acc: 0.6226 | Val   Acc: 0.7998 |Loss: 0.4082 |\n",
      "Epoch 242 | Train Acc: 0.5971 | Val   Acc: 0.8021 |Loss: 0.4181 |\n",
      "Epoch 243 | Train Acc: 0.6808 | Val   Acc: 0.8025 |Loss: 0.4060 |\n",
      "Epoch 244 | Train Acc: 0.6272 | Val   Acc: 0.7994 |Loss: 0.4438 |\n",
      "Epoch 245 | Train Acc: 0.6129 | Val   Acc: 0.8012 |Loss: 0.4176 |\n",
      "Epoch 246 | Train Acc: 0.6102 | Val   Acc: 0.7998 |Loss: 0.4261 |\n",
      "Epoch 247 | Train Acc: 0.6650 | Val   Acc: 0.7994 |Loss: 0.4110 |\n",
      "Epoch 248 | Train Acc: 0.6714 | Val   Acc: 0.8029 |Loss: 0.4158 |\n",
      "Epoch 249 | Train Acc: 0.5846 | Val   Acc: 0.7998 |Loss: 0.4411 |\n",
      "Epoch 250 | Train Acc: 0.6426 | Val   Acc: 0.8012 |Loss: 0.4038 |\n",
      "best acc: 0.8043\n",
      "Training time: 196.79 minutes\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 6 # TRAINING\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = train(model,\n",
    "              train_loader,\n",
    "              val_loader,\n",
    "              optimizer,\n",
    "              scheduler,\n",
    "              loss_function,\n",
    "              device,\n",
    "              num_epochs=NUM_EPOCHS,\n",
    "              patience=PATIENCE)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = (end_time - start_time) / 60\n",
    "print(f\"Training time: {elapsed_time:.2f} minutes\")\n",
    "\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6affd70e",
   "metadata": {
    "_cell_guid": "72f96538-261e-4e39-a2cf-9c59633cff44",
    "_uuid": "c6c29f82-4520-4373-bc8e-ac7133cd1102",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-30T19:06:21.351744Z",
     "iopub.status.busy": "2025-04-30T19:06:21.351038Z",
     "iopub.status.idle": "2025-04-30T19:06:47.413548Z",
     "shell.execute_reply": "2025-04-30T19:06:47.412605Z"
    },
    "id": "L38TEz0f1bNP",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "058dbb92-3dda-47ce-9ad6-2f25144d7424",
    "papermill": {
     "duration": 26.077547,
     "end_time": "2025-04-30T19:06:47.415090",
     "exception": false,
     "start_time": "2025-04-30T19:06:21.337543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  label\n",
      "0  22430      0\n",
      "1  22431      8\n",
      "2  22432     10\n",
      "3  22433      6\n",
      "4  22434      8\n",
      "5  22435     10\n",
      "6  22436      3\n",
      "7  22437      5\n",
      "8  22438     13\n",
      "9  22439     17\n",
      "(4000, 2)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 7 # TESTING\n",
    "\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, img_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        preds = model(images)\n",
    "        predicted_labels = preds.argmax(dim=1).cpu().numpy()\n",
    "        results.extend(zip(img_ids, predicted_labels))\n",
    "\n",
    "submission_df = pd.DataFrame(results, columns=['id', 'label'])\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(submission_df.head(10))\n",
    "print(submission_df.shape)\n",
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11751646,
     "sourceId": 96834,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11861.50372,
   "end_time": "2025-04-30T19:06:50.546853",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-30T15:49:09.043133",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
